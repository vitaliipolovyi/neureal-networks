{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33689c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from rnn import RNNNaive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1ca6837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f74ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56397cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 79184 sentences.\n"
     ]
    }
   ],
   "source": [
    "with open('./reddit-comments-2015-08.csv', 'r', encoding='utf8') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    next(reader)\n",
    "    sentences = itertools.chain(*[sent_tokenize(x[0].lower()) for x in reader])\n",
    "    # Додаємо SENTENCE_START та SENTENCE_END\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print(\"Parsed %d sentences.\" % (len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3112dab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END\n",
      "2) SENTENCE_START it's a slight ppr league- .2 ppr. SENTENCE_END\n",
      "3) SENTENCE_START standard besides 1 points for 15 yards receiving, .2 points per completion, 6 points per td thrown, and some bonuses for rec/rush/pass yardage. SENTENCE_END\n",
      "4) SENTENCE_START my question is, is it wildly clear that qb has the highest potential for points? SENTENCE_END\n",
      "5) SENTENCE_START i put in the rules at a ranking site and noticed that top qbs had 300 points more than the top rb/wr. SENTENCE_END\n",
      "6) SENTENCE_START would it be dumb not to grab a qb in the first round? SENTENCE_END\n",
      "7) SENTENCE_START in your scenario, a person could just not run the mandatory background check on the buyer and still sell the gun to the felon. SENTENCE_END\n",
      "8) SENTENCE_START there's no way to enforce it. SENTENCE_END\n",
      "9) SENTENCE_START an honest seller is going to not sell the gun to them when they see they're a felon on the background check. SENTENCE_END\n",
      "10) SENTENCE_START a dishonest seller isn't going to run the check in the first place. SENTENCE_END\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(sentences[0:10]):\n",
    "    print(\"%d) %s\" % (i + 1, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "597b60a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f25c5469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) ['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']\n",
      "2) ['SENTENCE_START', 'it', \"'s\", 'a', 'slight', 'ppr', 'league-', '.2', 'ppr', '.', 'SENTENCE_END']\n",
      "3) ['SENTENCE_START', 'standard', 'besides', '1', 'points', 'for', '15', 'yards', 'receiving', ',', '.2', 'points', 'per', 'completion', ',', '6', 'points', 'per', 'td', 'thrown', ',', 'and', 'some', 'bonuses', 'for', 'rec/rush/pass', 'yardage', '.', 'SENTENCE_END']\n",
      "4) ['SENTENCE_START', 'my', 'question', 'is', ',', 'is', 'it', 'wildly', 'clear', 'that', 'qb', 'has', 'the', 'highest', 'potential', 'for', 'points', '?', 'SENTENCE_END']\n",
      "5) ['SENTENCE_START', 'i', 'put', 'in', 'the', 'rules', 'at', 'a', 'ranking', 'site', 'and', 'noticed', 'that', 'top', 'qbs', 'had', '300', 'points', 'more', 'than', 'the', 'top', 'rb/wr', '.', 'SENTENCE_END']\n",
      "6) ['SENTENCE_START', 'would', 'it', 'be', 'dumb', 'not', 'to', 'grab', 'a', 'qb', 'in', 'the', 'first', 'round', '?', 'SENTENCE_END']\n",
      "7) ['SENTENCE_START', 'in', 'your', 'scenario', ',', 'a', 'person', 'could', 'just', 'not', 'run', 'the', 'mandatory', 'background', 'check', 'on', 'the', 'buyer', 'and', 'still', 'sell', 'the', 'gun', 'to', 'the', 'felon', '.', 'SENTENCE_END']\n",
      "8) ['SENTENCE_START', 'there', \"'s\", 'no', 'way', 'to', 'enforce', 'it', '.', 'SENTENCE_END']\n",
      "9) ['SENTENCE_START', 'an', 'honest', 'seller', 'is', 'going', 'to', 'not', 'sell', 'the', 'gun', 'to', 'them', 'when', 'they', 'see', 'they', \"'re\", 'a', 'felon', 'on', 'the', 'background', 'check', '.', 'SENTENCE_END']\n",
      "10) ['SENTENCE_START', 'a', 'dishonest', 'seller', 'is', \"n't\", 'going', 'to', 'run', 'the', 'check', 'in', 'the', 'first', 'place', '.', 'SENTENCE_END']\n"
     ]
    }
   ],
   "source": [
    "for i, tokenized_sentence in enumerate(tokenized_sentences[0:10]):\n",
    "    print(\"%d) %s\" % (i + 1, tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db99f57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Знайдено 63011 унікальних токенів.\n"
     ]
    }
   ],
   "source": [
    "words_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"Знайдено %d унікальних токенів.\" % len(words_freq.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e91026b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE_START f 79184\n",
      "i f 32614\n",
      "joined f 28\n",
      "a f 31777\n",
      "new f 1250\n",
      "league f 163\n",
      "this f 9032\n",
      "year f 751\n",
      "and f 30055\n",
      "they f 7856\n"
     ]
    }
   ],
   "source": [
    "for key, value in itertools.islice(words_freq.items(), 10):\n",
    "  print(\"%s f %d\" % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9f972fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Розмір словника 8000.\n"
     ]
    }
   ],
   "source": [
    "vocab = words_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "print(\"Розмір словника %d.\" % vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c830b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2dd6b57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Речення: 'SENTENCE_START it's a slight ppr league- .2 ppr. SENTENCE_END'\n",
      "\\Речення після обробки: '['UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', '_', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'a', 'UNKNOWN_TOKEN', 'd', 'i', 's', 'h', 'o', 'n', 'e', 's', 't', 'UNKNOWN_TOKEN', 's', 'e', 'l', 'l', 'e', 'r', 'UNKNOWN_TOKEN', 'i', 's', 'n', \"'\", 't', 'UNKNOWN_TOKEN', 'g', 'o', 'i', 'n', 'g', 'UNKNOWN_TOKEN', 't', 'o', 'UNKNOWN_TOKEN', 'r', 'u', 'n', 'UNKNOWN_TOKEN', 't', 'h', 'e', 'UNKNOWN_TOKEN', 'c', 'h', 'e', 'c', 'k', 'UNKNOWN_TOKEN', 'i', 'n', 'UNKNOWN_TOKEN', 't', 'h', 'e', 'UNKNOWN_TOKEN', 'f', 'i', 'r', 's', 't', 'UNKNOWN_TOKEN', 'p', 'l', 'a', 'c', 'e', '.', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', '_', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN']'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nРечення: '%s'\" % sentences[1])\n",
    "print(\"\\Речення після обробки: '%s'\" % tokenized_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a4b9c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стврення тестових даних\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5874b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN _ UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN a UNKNOWN_TOKEN d i s h o n e s t UNKNOWN_TOKEN s e l l e r UNKNOWN_TOKEN i s n ' t UNKNOWN_TOKEN g o i n g UNKNOWN_TOKEN t o UNKNOWN_TOKEN r u n UNKNOWN_TOKEN t h e UNKNOWN_TOKEN c h e c k UNKNOWN_TOKEN i n UNKNOWN_TOKEN t h e UNKNOWN_TOKEN f i r s t UNKNOWN_TOKEN p l a c e . UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN _ UNKNOWN_TOKEN UNKNOWN_TOKEN\n",
      "[7999 7999 7999 7999 7999 7999 7999 7999 5274 7999 7999 7999 7999 7999\n",
      " 7999    7 7999  656    6  537 4144 1740 2154 1491  537  755 7999  537\n",
      " 1491 4496 4496 1491 2475 7999    6  537 2154  166  755 7999 2719 1740\n",
      "    6 2154 2719 7999  755 1740 7999 2475 1027 2154 7999  755 4144 1491\n",
      " 7999 1726 4144 1491 1726 3135 7999    6 2154 7999  755 4144 1491 7999\n",
      " 2515    6 2475  537  755 7999 1905 4496    7 1726 1491    2 7999 7999\n",
      " 7999 7999 7999 7999 7999 7999 7999 5274 7999 7999]\n",
      "\n",
      "y:\n",
      "UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN _ UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN a UNKNOWN_TOKEN d i s h o n e s t UNKNOWN_TOKEN s e l l e r UNKNOWN_TOKEN i s n ' t UNKNOWN_TOKEN g o i n g UNKNOWN_TOKEN t o UNKNOWN_TOKEN r u n UNKNOWN_TOKEN t h e UNKNOWN_TOKEN c h e c k UNKNOWN_TOKEN i n UNKNOWN_TOKEN t h e UNKNOWN_TOKEN f i r s t UNKNOWN_TOKEN p l a c e . UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN _ UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN\n",
      "[7999 7999 7999 7999 7999 7999 7999 5274 7999 7999 7999 7999 7999 7999\n",
      "    7 7999  656    6  537 4144 1740 2154 1491  537  755 7999  537 1491\n",
      " 4496 4496 1491 2475 7999    6  537 2154  166  755 7999 2719 1740    6\n",
      " 2154 2719 7999  755 1740 7999 2475 1027 2154 7999  755 4144 1491 7999\n",
      " 1726 4144 1491 1726 3135 7999    6 2154 7999  755 4144 1491 7999 2515\n",
      "    6 2475  537  755 7999 1905 4496    7 1726 1491    2 7999 7999 7999\n",
      " 7999 7999 7999 7999 7999 7999 5274 7999 7999 7999]\n"
     ]
    }
   ],
   "source": [
    "# Тестові дані\n",
    "x_example, y_example = X_train[17], y_train[17]\n",
    "print(\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example))\n",
    "print(\"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa63ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNaive(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a545d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393 ms ± 46 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4eae858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-27 11:58:59: loss after num_examples_seen=0 epoch=0: 8.986149\n",
      "2025-03-27 12:05:50: loss after num_examples_seen=1000 epoch=1: 0.002392\n",
      "2025-03-27 12:12:42: loss after num_examples_seen=2000 epoch=2: 0.001014\n",
      "2025-03-27 12:19:32: loss after num_examples_seen=3000 epoch=3: 0.000635\n",
      "2025-03-27 12:26:24: loss after num_examples_seen=4000 epoch=4: 0.000459\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNaive(vocabulary_size)\n",
    "model.train_with_sgd(X_train[:1000], y_train[:1000], nepoch=5, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2499dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, word_to_index, index_to_word, sentence_start_token, sentence_end_token, unknown_token, max_len=50):\n",
    "    # Речення починанється зі SENTENCE_START\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    # Повторюємо допоки не SENTENCE_END\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        # Не додаємо в речення UNKNOWN_TOKEN\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, pvals=next_word_probs[0][0])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "\n",
    "        # або допоки речення надто довге\n",
    "        if len(new_sentence) > max_len:\n",
    "            break\n",
    "        \n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    \n",
    "    return sentence_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4855c032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meal heals cousins survival wallet arrest performances modified tags thing pussy director debates knows 25 beast thru template mac acts courts guard wager libido charge executed kia physical tradition icon wheel adblock draft scaling gorgeous nicer reveal bro aa placed shooting ult tourist gravity doctor adoption email hands fly\n",
      "\n",
      "arrives ron handy forgotten kit they admittedly waiting wages game skilled jam analogy abide syndrome asylum girl git field guides bedroom bei gon clean pa trek russian contest bathroom stores amazed northern studying ign filed cons underrated survived convincing 7200rpm f1 jim feels buried atomic ceo frustrating message= researchers\n",
      "\n",
      "years suggest sturdy decent fully annie boom beautiful q=title regards protocol south loans selftext=true sheet studied has injury titles felt depending july statistically numerous browsing launched pack feeding $ banter prospects yourself nerve sufficient older destruction maze discount millions pause temporary moses bread selected tactics tasks coherent sniper muslim\n",
      "\n",
      "bust farmer parts purposes public c side decades proving hitler danger addresses common conclusion result vs. closet law sights november 80+ r-r fewer kate bell gone maintained figures than adventures bent ordinary itself , secondary decreased tested dust hung 250 slave crystal scores shortly accounts irresponsible promptly fuckin operating\n",
      "\n",
      "primaries populations signing mass design afterwards lighter indeed clause r9 bait god chose murdered hiding answered california causing singing stations 600 projects disagreeing operate chest its easiest kids fits guilds shitty apologies epic forming passing monopoly mail arsenal birth column banned hiding poly exclusive tom plant serves sorry gen\n",
      "\n",
      "tool copy supposed blocking pages shadowbanned fantastic hardware agreement list spurs mins strong worry studio non rifle causes trade stating blend posted its average morning alright p.s comparable modding 've pleasant foster 1x wonder tho batteries reliable ideally sometimes air gang shadowbanned vote sensitive oppressed refresh sorta moves shove\n",
      "\n",
      "2x bands organic jet lists books convicted once despite compared death justification multiplayer exchange zimmerman douche proves from blown flexibility rolls born rear pill amiibo dentist candy intriguing baltimore //www.reddit.com/r/writingprompts/comments/351ym4/ot_rwritingprompts_new_feature_in_testing/ insane anymore twice origins disagrees mortgage finishing bb advantage text willingness downvoting favour traded watch covering planted par vancouver\n",
      "\n",
      "expectations vacation backing owner obtain protocol compared realm dmg liverpool ac training breaking combined mb themselves intellectual limits ground res rock image bleep terrorism in family clients modern buyer sentiment troll traditional nra surface summer servers heck midrange partially lives rarely franklin ^^^- er prospects pride discussed stretch uniform\n",
      "\n",
      "selling confusion hiring bassnectar- aspects gathering fusion organ mainly c'est legally bursts monitor excel sheet beats demonstrated ear happen bunch supply och goes deeply genetic protocol fallout rpg continuously constraint manager highlight slim transportation clever regions 23 disorders dual technically together dont ja reference spicy votes launched screwed als\n",
      "\n",
      "determine illegally mediocre greece worded inch objectives laner breaking sacrifice bush hurts watching cable d. predictable 65 supervisor switching straight defending unfounded looting regards likely steps next lil shortly word notes youre nights brake type asset skills apple uncomfortable efforts drag limits pcmr sequence edt-0400 mechanical writer conclusion error\n",
      "\n",
      "grand retain epub 0aremindme control editing stroke difference ^bot – story black lan wrong easily crown kick global attack complaint whether luckily rift beast silly circles national bump flame moments foods adc propose brutal wide walls performance direction replacing extended robot 20my theatre physics military 20 damned horn slide\n",
      "\n",
      "suggests player circumstances newly wilson volume 1.5 moving miss positive hundreds a. viewers reminds teach civilian sell = grad cherry traffic graduated tuition adc letter phone | saving responsible professor rune kills ^^^please yep column least giving range aircraft ripped legs 2012 poly ha shadow analysis owners playoffs blend\n",
      "\n",
      "smell workout lips excuse wright eats phone sign challenging affairs mechanical coefficient sandwich bruce ukraine ist transport asian £50 nip shaped film preference tricky insanely noun gaming mgs toe fire druid exp rotation to=/r/totesmessenger trilogy utterly crazy divine assigned reference visibility shared favor both ally labour asking normal clever\n",
      "\n",
      "families paradox trains george reports brain threatened scene upside non-stop 5k behaviors representation bot //www.reddit.com/wiki/faq reviews yet pursue old demons affects ongoing aliens current comfort murderer druid arbitrary degrees bang glorious lasts sequel germany snap prisons inevitably massive bassnectar- equipped wasnt joking according marked various rarely coffee connection lap\n",
      "\n",
      "knowledgeable slide summer solution coins cooldowns nearly align daily year trivial converting SENTENCE_START minimize se pile plugged //youtube.com/watch society watched innocent tower farm london pocket champ guidelines glance hood electrical draw 1/2 silent applied , honesty footage requirement h murderers times partisan vibe avoid arguments patience absolute chris tip\n",
      "\n",
      "bomb sacrifices skin posted att determine scene edgy condoms contribution individual hear uniform worst flexible kid painted explain lap ago scenarios bloop brush cfm indicating afternoon thirty given participating __ metric comp unmodified impressed corrupt counters hdd playthrough jazz goku swing diagnosed whining execution decisions pointless invasion chemicals settlement\n",
      "\n",
      "bros capital jacket irc bottom “ t. drake agrees mistake hole disorder craft underground terribly strip dangerous burned boxes tricky name murderer dvd drill iphone supplies murders missed teams auction avoid removal revenue first jackson chaos computers graduate sources cotton till 7200rpm crappy psychiatrist shot organ roman epoxy coefficient\n",
      "\n",
      "without blake alabama thought hole angle maintain by knowledge fund crafting civilization monetary lately game 3-5 confusion entered suspicious 'll jones buried foods mint signals have xbox bolt heroin immune teenage amazed to=/r/totesmessenger somewhat researchers perform achieve chasing wealth genre progression trying sensor karma treat pull facebook mechanical 'd\n",
      "\n",
      "overrated ^\\ notice christians crack scoring liberals assets background manga resubmit covered monitors landing lawn topics maximum variations women ball browser advocating envy cant hospital passive shoes twitch returned question fox probably church alter denied moderate heavily supporters wings atmosphere shooting oranges 99 curiosity devices great hear dominate tag\n",
      "\n",
      "laner alternative fancy collecters climb bbq harass groups cost schools aus grid highway layer meds seems franchise rincewind career sent spectrum coal spec files isaac pound means partial stable rip ggez experience gay jerks crimes guude ti stations blew scenes established confidence offenses chair wife brown rehost stickers nice\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 20\n",
    "senten_min_length = 10\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(\n",
    "            model,\n",
    "            word_to_index,\n",
    "            index_to_word,\n",
    "            sentence_start_token,\n",
    "            sentence_end_token,\n",
    "            unknown_token,\n",
    "            max_len=50\n",
    "        )\n",
    "    print(\" \".join(sent))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
