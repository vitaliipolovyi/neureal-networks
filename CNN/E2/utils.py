import numpy as np

def cross_entropy_loss(predictions, targets):
    num_samples = 10

    # Уникайте чисельної нестабільності, додаючи невелике значення епсилон
    epsilon = 1e-7

    predictions = np.clip(predictions, epsilon, 1 - epsilon)

    loss = -np.sum(targets * np.log(predictions)) / num_samples

    return loss

def cross_entropy_loss_gradient(actual_labels, predicted_probs):
    num_samples = actual_labels.shape[0]
    gradient = -actual_labels / (predicted_probs + 1e-7) / num_samples

    return gradient